package org.ferrit.core.crawler

import scala.util.{Failure, Success, Try}
import org.ferrit.core.filter.UriFilter
import org.ferrit.core.uri.CrawlUri
import org.ferrit.core.crawler.CrawlConfig._

case class CrawlConfig(

  /**
    * The unique identifier of the org.ferrit.core.crawler configuration.
    */
  id: String,

  /**
    * The User-Agent header sent to web servers during a fetch to identify
    * the org.ferrit.core.crawler. This must set this before starting a crawl. If not set,
    * the default user-agent property in application.conf is used instead.
    */
  userAgent: Option[String],

  /**
    * The seeds are the starting points of a crawl and should
    * be ordinary URLs not regex patterns. Additional
    * follow patterns are needed to accept the seeds for following.
    * Adding seeds by themselves are not enough.
    */
  seeds: Seq[CrawlUri],

  /**
    * Configuration for the UriFilter used by the org.ferrit.core.crawler.
    */
  uriFilter: UriFilter,

  /**
    * One or more test cases to validate the UriFilter rules.
    */
  tests: Option[Seq[String]],

  /**
    * The name of the crawl used for identification in crawl logs and job monitoring tools.
    * For example, the name of the website being crawled or meaningful group
    * name when crawling multiple sites.
    */
  crawlerName: String,

  /**
    * A custom politeness setting to use if the 'Crawl-delay' directive is
    * not found in the robots.txt file.
    */
  crawlDelayMillis: Int,

  /**
    * The length of time a crawl can run before quitting.
    */
  crawlTimeoutMillis: Long,

  /**
    * Set the maximum depth of the crawl, which is to say the number of
    * links away from the seed URIs. A depth of 0 means that only the
    * seed URIs will be crawled.
    */
  maxDepth: Int,

  /**
    * The maximum number of fetches that the org.ferrit.core.crawler can make during the crawl.
    */
  maxFetches: Int,

  /**
    * The maximum number of URLs that can be queued for fetching.
    * The org.ferrit.core.crawler will quit if the number of queued URLs exceeds this value.
    * This is to prevent out-of-memory exceptions and alert you about larger than
    * expected queue sizes.
    */
  maxQueueSize: Int,

  /**
    * The % percentage of request fails tolerated before the org.ferrit.core.crawler gives up.
    * This prevents misconfigured crawlers from running on indefinitely.
    * Too many 404 errors could indicate that the seed URIs are incorrect.
    * Too many 500 errors could indicate an over-eager org.ferrit.core.crawler or a server
    * trying to ban the org.ferrit.core.crawler.
    */
  maxRequestFails: Double

                      ) {

  def getUserAgent: String = userAgent.getOrElse("")

  def validated: Try[Boolean] = {

    def because(reason: String) = CrawlRejectException(reason)

    if (getUserAgent.trim.isEmpty) {
      Failure(because(UserAgentMissing))

    } else if (seeds == null || seeds.isEmpty) {
      Failure(because(SeedsAreMissing))

    } else if (!seeds.forall(uriFilter.accept)) {
      val explanation = seeds.map(uriFilter.explain).mkString(" and ")
      Failure(because(SeedsAreRejected.format(explanation)))

    } else {
      Success(true)
    }

  }

}

object CrawlConfig {

  val UserAgentMissing =
    "Please set a sensible userAgent value in your org.ferrit.core.crawler configuration." +
      "This is used to form a User-agent header during HTTP fetch requests " +
      "to help identify your org.ferrit.core.crawler to the web server. Some web servers will " +
      "block requests if a user agent is not defined. " +
      "The user agent is also used when evaluating Robots Exclusion rules."

  val SeedsAreMissing =
    "seed URIs are required to start a org.ferrit.core.crawler"

  val SeedsAreRejected =
    "The org.ferrit.core.crawler URI filter rejected the seeds: %s"

}

